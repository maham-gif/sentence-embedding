{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d44dfb9",
   "metadata": {},
   "source": [
    "# Sentence Embedding Lab\n",
    "\n",
    "This lab demonstrates sentence embeddings using SentenceTransformers, cosine similarity, DistilBERT, FAISS vector search, and PCA visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72838776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the pre-trained sentence embedding model (all-MiniLM-L6-v2)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Example sentences to embed\n",
    "sentences = [\n",
    "    \"Machine learning is changing the world.\",\n",
    "    \"Artificial intelligence and machine learning revolutionize industries.\",\n",
    "    \"The cat sits on the mat.\",\n",
    "    \"A kitten is sitting on a rug.\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(sentences, normalize_embeddings=True)\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950d3b3",
   "metadata": {},
   "source": [
    "## Cosine Similarity between Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1349289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Cosine similarity between sentence 0 and 1 (which are about ML/AI)\n",
    "cos_sim_0_1 = np.dot(embeddings[0], embeddings[1])\n",
    "# Cosine similarity between sentence 2 and 3 (both about cat on mat)\n",
    "cos_sim_2_3 = np.dot(embeddings[2], embeddings[3])\n",
    "print(\"Cosine sim between 0 and 1:\", cos_sim_0_1)\n",
    "print(\"Cosine sim between 2 and 3:\", cos_sim_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1535d",
   "metadata": {},
   "source": [
    "## Using DistilBERT for Manual Sentence Embeddings (Mean Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd130cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "distilbert_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(distilbert_name)\n",
    "model_distilbert = AutoModel.from_pretrained(distilbert_name)\n",
    "\n",
    "sentence = \"Machine learning is changing the world.\"\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "token_ids = inputs['input_ids']\n",
    "mask = inputs['attention_mask']\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_distilbert(token_ids, attention_mask=mask)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "expanded_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "summed = torch.sum(token_embeddings * expanded_mask, dim=1)\n",
    "counts = torch.clamp(expanded_mask.sum(dim=1), min=1e-9)\n",
    "sentence_embedding = (summed / counts).squeeze()\n",
    "\n",
    "print(\"DistilBERT embedding vector:\", sentence_embedding[:5], \"...\")\n",
    "print(\"Length:\", sentence_embedding.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aa9e62",
   "metadata": {},
   "source": [
    "## Creating a FAISS Index for Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "corpus = [\n",
    "    \"Machine learning is changing the world of technology and society.\",\n",
    "    \"Artificial intelligence and machine learning are revolutionizing many industries.\",\n",
    "    \"Cats and dogs are common household pets.\",\n",
    "    \"A kitten was found sitting on a carpet.\"\n",
    "]\n",
    "\n",
    "corpus_embeddings = model.encode(corpus, normalize_embeddings=True)\n",
    "d = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(corpus_embeddings)\n",
    "print(\"FAISS index size:\", index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484d7e0",
   "metadata": {},
   "source": [
    "## Querying the FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is AI impacting different sectors?\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "k = 2\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(\"Nearest neighbors indices:\", indices)\n",
    "print(\"Nearest neighbors scores:\", distances)\n",
    "for idx in indices[0]:\n",
    "    print(\"Retrieved doc:\", corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a359fdc5",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_embeddings = np.vstack([embeddings, corpus_embeddings])\n",
    "pca = PCA(n_components=2)\n",
    "emb_2d = pca.fit_transform(all_embeddings)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(emb_2d[:len(sentences), 0], emb_2d[:len(sentences), 1], c='blue', label='Query sentences')\n",
    "plt.scatter(emb_2d[len(sentences):, 0], emb_2d[len(sentences):, 1], c='red', marker='^', label='Corpus docs')\n",
    "for i, txt in enumerate(sentences + corpus):\n",
    "    plt.annotate(f\"{i}\", (emb_2d[i,0]+0.1, emb_2d[i,1]+0.1))\n",
    "plt.legend()\n",
    "plt.title(\"PCA projection of sentence embeddings\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
